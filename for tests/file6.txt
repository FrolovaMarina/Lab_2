What is a computer? A computer is an electronic machine that processes information—in other words, an information processor: it takes in raw information (or data) at one end, stores it until it's ready to work on it, chews and crunches it for a bit, then spits out the results at the other end. Overall, a computer works in four steps: Input: Input is the data before processing. It comes from the mouse, keyboard, microphone, and other external sensors. Storage: The storage is how the computer retains input data. The hard drive is used for long-term and mass data storage while the data set for immediate processing is stored temporarily in the Random Access Memory (RAM). Processing: Processing is where input gets transformed into output. The computer's Central Processing Unit (CPU) is its brain. It's responsible for executing instructions and performing mathematical operations on the input data. Output: Output is the final result of data processing. It can be anything from images, video, or audio content, even the words you type using a keyboard. You can also receive the output through a printer or a projector instead of directly through your device. What is a computer program? The first computers were gigantic calculating machines and all they ever really did was "crunch numbers": solve lengthy, difficult, or tedious mathematical problems. Today, computers work on a much wider variety of problems—but they are all still, essentially, calculations. Everything a computer does, from helping you to edit a photograph you've taken with a digital camera to displaying a web page, involves manipulating numbers in one way or another. What makes a computer different from a calculator is that it can work all by itself. You just give it your instructions (called a program) and off it goes, performing a long and complex series of operations all by itself. Back in the 1970s and 1980s, if you wanted a home computer to do almost anything at all, you had to write your own little program to do it. For example, before you could write a letter on a computer, you had to write a program that would read the letters you typed on the keyboard, store them in the memory, and display them on the screen. Writing the program usually took more time than doing whatever it was that you had originally wanted to do. Pretty soon, people started selling programs like word processors to save the need to write programs yourself. Today, most computer users rely on prewritten programs like Microsoft Word and Excel. (apps are just very neatly packaged computer programs) Most people see their computers as tools that help them do jobs, rather than complex electronic machines they have to pre-program. What's the difference between hardware and software? The beauty of a computer is that it can run a word-processing program one minute—and then a photo-editing program five seconds later. In other words, although we don't really think of it this way, the computer can be reprogrammed as many times as you like. This is why programs are also called software. They're "soft" in the sense that they are not fixed: they can be changed easily. By contrast, a computer's hardware—the bits and pieces from which it is made (and the peripherals, like the mouse and printer, you plug into it)—is pretty much fixed when you buy it off the shelf. The hardware is what makes your computer powerful; the ability to run different software is what makes it flexible. What is an operating system? Suppose you're back in the late 1970s, before off-the-shelf computer programs have really been invented. You want to program your computer to work as a word processor so you can bash out your first novel—which is relatively easy but will take you a few days of work. A few weeks later, you tire of writing things and decide to reprogram your machine so it'll play chess. Later still, you decide to program it to store your photo collection. Every one of these programs does different things, but they also do quite a lot of similar things too. For example, they all need to be able to read the keys pressed down on the keyboard, store things in memory and retrieve them, and display characters (or pictures) on the screen. If you were writing lots of different programs, you'd find yourself writing the same bits of programming to do these same basic operations every time. That's a bit of a programming chore, so why not simply collect together all the bits of program that do these basic functions and reuse them each time? You can think of a computer as a series of layers, with the hardware at the bottom, the BIOS connecting the hardware to the operating system, and the applications you actually use (such as word processors, Web browsers, and so on) running on top of that. Each of these layers is relatively independent so, for example, the same Windows operating system might run on laptops running a different BIOS, while a computer running Windows (or another operating system) can run any number of different applications. That's the basic idea behind an operating system: it's the core software in a computer that controls the basic chores of input, output, storage, and processing. You can think of an operating system as the "foundations" of the software in a computer that other programs are built on top of. So a word processor and a chess game are two different applications that both rely on the operating system to carry out their basic input, output, and so on. The operating system relies on an even more fundamental piece of programming called the BIOS (Basic Input Output System), which is the link between the operating system software and the hardware. The BIOS is a program semi-permanently stored into one of the computer's main chips, so it's known as firmware (it is usually designed so it can be updated occasionally, however). Operating systems have another big benefit. Back in the 1970s (and early 1980s), virtually all computers ran in their own, idiosyncratic ways with fairly unique hardware (different processor chips, memory addresses, screen sizes and all the rest). Programs written for one machine (such as an Apple) usually wouldn't run on any other machine without quite extensive conversion. That was a big problem for programmers because it meant they had to rewrite all their programs each time they wanted to run them on different machines. If you have a standard operating system and you tweak it so it will work on any machine, all you have to do is write applications that work on the operating system. Then any application will work on any machine. The operating system that definitively made this breakthrough was Microsoft Windows, spawned by Bill Gates. (But it's important to note that there were earlier operating systems too.)...HISTORY OF 3D MODELING: FROM EUCLID TO 3D PRINTING History of 3D modeling went through many developments on its way to being wrapped around our everyday lives. Paradoxically, the history of 3D modeling began way before the first PC appeared. It all started with mathematical ideas that are behind 3D visualization. In fact, some of the basic ideas came from Euclid, sometimes referred to as the “founder of geometry”, who lived in the 3rd century BC. Then, Rene Descartes in the 1600s gave the world analytic geometry, also known as coordinate geometry, which allowed to accurately track distances and locations. Later in the mid 18th century, English mathematician James Joseph Sylvester invented matrix mathematics which is now at work for every computer-generated image where one can see reflections or light distortion. In the 1950s, computers were developed and put to many mathematical uses – mainly military and scientific. But this led to the day in history when someone thought of putting one to do a realistic simulation of products and designs. The first advancements in the history of 3D modeling came when the first commercially available CAD or Computer Aided Design systems started coming out in the 1960s. The biggest breakthrough came from Ivan Sutherland who introduced Sketchpad, also known as “Robot Draftsman,” in 1963 with its revolutionary interface. Sketchpad established that computers can be used not only for engineering or repetitive drafting but interactively by Designers and Artists. In the same year, the General Motors and IBM partnership created the DAC-1, Design Augmented by Computer, which was publicly presented in 1964 and used by General Motors for the rest of the decade to speed up their car production workflow. It demonstrated that computer design visualization can cut down workloads which would’ve taken forever if the drawing boards were used. At the end of the decade, in 1968, Ivan Sutherland and David Evans founded the first 3D graphics company, “Evans & Sutherland”. They started the company to produce hardware to run the systems that were being developed but were soon also developing software. Their appearance and success in the market inspired others to start up their own companies and work on developing the technology. At this point in the history of 3D modeling, new companies began to offer automated design and drafting systems. ADAM, a CAD system released in 1971, was one of those. ADAM was designed to work on as many machines as possible, thus creating a huge spike in the availability of CAD which continued as computers and software grew more sophisticated. An item of interest was the Utah teapot model. It appeared in history as the symbol for 3D computer graphics after it was used by Martin Newell to test his graphical research. He found the 3D teapot model ideal for testing because of its structure, variety of surfaces it possessed and the item’s ability to cast shadows on itself. He shared the details with his fellow researchers who immediately started to use the teapot too. The introduction of the first IBM PC in 1981 caused a widespread use of CAD in not only aerospace and automotive industries but also in commercial engineering businesses. Solid 3D modeling then developed and became mainstream with the development of software such as Unigraphics’ UniSolids CAD. Later, in 1983, the 2D system AutoCAD was released. It was the first significant CAD program in the history of 3D modeling for the IBM PC, as it offered almost as much functionality of other CAD programs to date but at 20% of their cost. At this point in the history of 3D modeling, CAD software was widespread and being tested to its limits. It was now easier and cheaper to access professional programs and hardware allowing for any company, freelancer and hobbyist to create 3D models. The software was now constantly developing and changing, allowing for faster access and a more user-friendly experience. Meanwhile, free and open source software like Blender were popping up, so people fascinated with 3D learned and tried modeling for themselves. Popularization ensued as the Internet got flooded by hobbyists creating their work and even selling it – all leading up to where we are now. 3D modeling for printing was developed to account for the interest magazines had in using 3D models for articles. A separate part of the history of 3D modeling is the invention of SLA or stereolithography, the production parts in a layer by layer fashion. It was thought up in 1984 although the first machine that utilized it came out in 1992. This SLA machine used an acrylic-based material named photopolymer. It gets hit with a UV laser to turn it into solid plastic. That same year another machine came out with its own version which used powder instead of liquid and was dubbed as SLS for selective laser sintering. At this point in the history of 3D modeling, all this was very new and companies were only realizing the potential of such technology. It was still expensive and had its flaws, such as warping of materials during printing. A lot of prototyping and developments ensued, especially in the field of medicine where startups fabricated organs and prosthetics using 3D printing technology. As history went on, new possibilities opened up with the expansion of the Internet. In 2005, 3D printing project, RepRap, was successfully developed and released in 2008. Dubbed “Darwin”, the 3D printer was able to print most of its own parts through the use of different filaments. This sparked major hype and enthusiasm for 3D printing, as people realized that anything they could think of could be printed right at home, even the printer. Following this huge spark of interest, Kickstarter funded countless 3D printing projects. These publicly funded projects brought about the flexibility and affordability that 3D printing now possesses. There are countless amazing stories about what 3D printing has done and is able to do today. They come from manufacturing housing and small precise electronics to organs and prosthetics. The whole history of 3D modeling came out of many mathematicians precisely describing the process of making and tracking geometry. This was found highly applicable in computers which, after some research, it was found to produce incredibly useful visualizations. Now with the development in this technology, 3D modeling and visualization are widely available and affordable as ever. Not only that, the photorealism it produces can be hard to tell from reality, and can even be 3D printed into reality... VR technology. Virtual reality is a completely computer generated simulation, where your entire view is virtual. This means that we can manipulate this virtual world to whatever we would like it to be, and produce some really amazing things. At it's core, VR technology has one purpose: to simulate settings and environments realistically enough to fool the human brain into accepting them as reality. From a scientific standpoint, that all begins by understanding how our brains interpret the things we see to develop a mental picture of the world around us. The simplest explanation is that our perception of reality is based on rules we develop using our experiences as a guide. For example, when we see the sky, it tells us which direction is "up". When we see objects we can identify, we can use their size relative to one another to judge distance. We can also detect light sources by picking up on the shadows cast by the objects around us. VR designers can use those conventional rules to create virtual environments that conform to our mental expectations of reality. A history of VR technology. The concept of virtual reality has been around for decades, even though the public really only became aware of it in the early 1990s. In the mid 1950s, a cinematographer named Morton Heilig envisioned a theatre experience that would stimulate all his audiences’ senses, drawing them in to the stories more effectively. He built a single user console in 1960 called the Sensorama that included a stereoscopic display, fans, odor emitters, stereo speakers and a moving chair. He also invented a head mounted television display designed to let a user watch television in 3-D. Users were passive audiences for the films, but many of Heilig’s concepts would find their way into the VR field. Philco Corporation engineers developed the first HMD in 1961, called the Headsight. The helmet included a video screen and tracking system, which the engineers linked to a closed circuit camera system. They intended the HMD for use in dangerous situations — a user could observe a real environment remotely, adjusting the camera angle by turning his head. Bell Laboratories used a similar HMD for helicopter pilots. They linked HMDs to infrared cameras attached to the bottom of helicopters, which allowed pilots to have a clear field of view while flying in the dark. In 1965, a computer scientist named Ivan Sutherland envisioned what he called the “UltimateDisplay.” Using this display, a person could look into a virtual world that would appear as real as the physical world the user lived in. This vision guided almost all the developments within the field of virtual reality. Virtual Reality can be considered an extremely immersive experience that we can enjoy safely and simulate something real. By using a Virtual Reality headset or applicable application, we can enter into special simulations. The viewer controls the environment and has the power to look around at any scene as well as potentially interact with the objects in the scene. Virtual Reality usually take place in computer-generated simulations in which an entire 3-D environment is rendered in the closest thing to a simulation. A person can then use a variety of electronic equipment to interact with the scene and control the environment. Overall, virtual reality means creating a simulation of a real world experience that a person can enter and leave at any time using technology. As VR continues to develop, we will only see these simulations getting more in-depth and real. The technology also continues to miniaturize and grow cheaper, bringing VR to a position where almost anyone could have the chance to try it. The technical basics of virtual reality: 1 - Field of View and Frame Rate. Field of view has been a common concern for VR developers. For VR to immerse us in a new environment, it needs to mimic our field of view. Unfortunately, human beings are capable of a much wider field of view in general than headsets can typically provide. The average human can see around 200-220 degrees of surrounding content. A VR headset can do around 180 degrees. Frame rate is the other visual element that defines how VR works. Frames need to move at an incredible pace within a VR headset screen to mimic what we see in real life. Experts believe that the human eye can handle up to 1000 frames per second. Most developers have found that anything less than 60 FPS causes feelings of disorientation and nausea. Experts are trying to push more towards 120 FPS. 2 - Spatial Audio and Sound Effects. A lot of people get caught up in the idea of VR being a visual experience. However, the reality is that VR tools attempt to immerse you fully into a different space. This demands more than just a good view of your surroundings. We also need spatial audio to help us feel like we’re in that new environment. The better the audio, the more immersed you feel, thanks to the sounds that seem to come from behind, above, or to the side of us. 3 - Position and Head Tracking. What makes VR truly engaging is the fact that we can move around in a virtual space, and that environment will adjust to your position. Head and position tracking features are measured in degrees of freedom, allowing we to explore either 6 degrees of freedom or 3. Headsets that use 6 degrees of freedom can check on our position in a room, and show the direction that your head is pointed towards. This means that we can have complete autonomous movement through a space. VR technology is growing increasingly impressive as we explore various ways to make the virtual world feel more like the real world. As developments continue, headsets are becoming sleeker and more mobile, with fewer wires and more high-definition pixels. Haptic sensors and tracking gloves are replacing clunky control systems, while machine learning and AI advancements change the way that VR technology tracks our interactions.
